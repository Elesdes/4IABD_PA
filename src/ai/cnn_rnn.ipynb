{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, Optional, Tuple, NoReturn\n",
    "\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import kerastuner_tensorboard_logger as kt_logger \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"../\"\n",
    "MONITORING = os.path.join(BASE_PATH, 'logs')\n",
    "DATA = os.path.join(BASE_PATH, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSORBOARD_LOG_DIR = os.path.join(MONITORING, \"tensorboard_logs\")\n",
    "CSV_LOG_DIR = os.path.join(MONITORING, \"csv_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "TUNERS = os.path.join(DATA, \"tuners\")\n",
    "MODELS = os.path.join(DATA, \"models\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GPU/TPU Multithreading Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "\n",
    "    strategy = tf.distribute.experimental.TPUStrategy\n",
    "except ValueError:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print('Number of replicas:', strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "    gpus = tf.config.experimental.list_logical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tpu:\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu, )\n",
    "    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
    "elif len(gpus) > 1:\n",
    "    strategy = tf.distribute.MultiWorkerMirroredStrategy([gpu.name for gpu in gpus])\n",
    "    print('Running on multiple GPUs ', [gpu.name for gpu in gpus])\n",
    "elif len(gpus) == 1:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print('Running on single GPU ', gpus[0].name)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print('Running on CPU')\n",
    "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjustable\n",
    "BATCH_SIZE = 32  # Big batch size, small learning rate\n",
    "HEIGHT, WIDTH = 224, 224\n",
    "IMG_SIZE = (HEIGHT, WIDTH)\n",
    "IMG_FORMAT = (HEIGHT, WIDTH, 3)\n",
    "EPOCHS = 100\n",
    "TRIALS = 100\n",
    "SEED = 42"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    'data/dataset',\n",
    "    validation_split=0.2,\n",
    "    seed=SEED,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(dataset.class_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = sklearn.model_selection.train_test_split(dataset, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1. / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_ds = train_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_dataset.cache().shuffle().refetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_dataset.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(hp: kt.HyperParameters) -> tf.keras.Sequential:\n",
    "    inputs = tf.keras.Input(shape=IMG_FORMAT, dtype=tf.float32)\n",
    "\n",
    "    # Conv & pooling tf.keras.layers\n",
    "    for i in range(num_layers := hp.Int('num_layers', min_value=0, max_value=3, step=1)):\n",
    "        for _ in range(2):\n",
    "            filters = hp.Int(f'filters_{i}',\n",
    "                             min_value=np.power(2, num_layers + i),\n",
    "                             max_value=np.power(2, (num_layers + 2) + i),\n",
    "                             step=8)\n",
    "            x = tf.keras.layers.Conv2D(filters=filters, kernel_size=(3, 3), activation='tanh', padding='same')(x)\n",
    "        x = tf.keras.layers.MaxPool2D()(x)\n",
    "\n",
    "    # Fully connected tf.keras.layers\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    for i in range(num_layers):\n",
    "        x = tf.keras.layers.Dense(units=hp.Int(f'units_{i}',\n",
    "                                               min_value=np.power(2, (num_layers * 2) - i),\n",
    "                                               max_value=np.power(2, np.power(2, num_layers) - i),\n",
    "                                               step=8),\n",
    "                                  activation='relu')(x)\n",
    "    outputs = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(hp: kt.HyperParameters) -> tf.keras.Sequential:\n",
    "    inputs = tf.keras.Input(shape=IMG_FORMAT, dtype=tf.float32)\n",
    "    for i in range(hp.Int('num_layers', min_value=0, max_value=3, step=1)):\n",
    "        x = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(\n",
    "                hp.Int(f'units_{str(i)}', min_value=8, max_value=64, step=8),\n",
    "                return_sequences=True,\n",
    "            )\n",
    "        )(x)\n",
    "    x = tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(hp.Int('lstm_units', min_value=8, max_value=64, step=8),\n",
    "                             return_sequences=False))(x)\n",
    "    x = tf.keras.layers.Dense(hp.Int('dense_units', min_value=8, max_value=64, step=8),\n",
    "                              activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs, x)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utilitary For Monitoring**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_exists(path: str) -> str:\n",
    "    if os.path.exists(path):\n",
    "        if path[-1].isdigit():\n",
    "            suffix = path[:path.rfind('_')]\n",
    "            digits = int(path[path.rfind('_') + 1:])\n",
    "            path = f\"{suffix}_{digits + 1}\"\n",
    "        else:\n",
    "            path = f\"{path}_0\"\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorboard_logs(model_name: str) -> tf.keras.callbacks.TensorBoard:\n",
    "    path = f\"{globals()[model_name.upper()]}\" \\\n",
    "           f\"_BS_{BATCH_SIZE}\" \\\n",
    "           f\"_LR_{SEED}\" \\\n",
    "           f\"_EPOCHS_{EPOCHS}\" \\\n",
    "           f\"_TRIALS_{TRIALS}\"\n",
    "    return tf.keras.callbacks.TensorBoard(path_exists(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epochs_logs(model_name: str) -> tf.keras.callbacks.CSVLogger:\n",
    "    path = f\"{globals()[model_name.upper()]}\" \\\n",
    "           f\"_BS_{BATCH_SIZE}\" \\\n",
    "           f\"_LR_{SEED}\" \\\n",
    "           f\"_EPOCHS_{EPOCHS}\" \\\n",
    "           f\"_TRIALS_{TRIALS}\"\n",
    "    return tf.keras.callbacks.CSVLogger(f\"{path_exists(path)}.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model: Any) -> NoReturn:\n",
    "    model_name = model.__name__\n",
    "    with strategy.scope():\n",
    "        tuner = tuner = kt.BayesianOptimization(model,\n",
    "                                        objective=kt.Objective('val_accuracy', direction='max'),\n",
    "                                        max_trials=TRIALS,\n",
    "                                        overwrite=True,\n",
    "                                        project_name=path_exists(f\"{TUNERS}\\\\{model_name}_tuner\"),\n",
    "                                        directory=path_exists(\"{TUNERS}_{model_name}\"))\n",
    "\n",
    "        # Search for best hyperparameters\n",
    "        tuner.search(train_dataset,\n",
    "                     epochs=EPOCHS,\n",
    "                     validation_data=val_dataset,\n",
    "                     callbacks=[stop_early,\n",
    "                                epochs_logs(model_name),\n",
    "                                tensorboard_logs(model_name)])\n",
    "        # Get the optimal hyperparameters\n",
    "        best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "        print(best_hps)\n",
    "\n",
    "        # Build model with optimal hyperparameters\n",
    "        model = tuner.hypermodel.build(best_hps)\n",
    "        history = model.fit(train_dataset,\n",
    "                            epochs=EPOCHS,\n",
    "                            validation_data=val_dataset,\n",
    "                            callbacks=[stop_early,\n",
    "                                       epochs_logs(model_name),\n",
    "                                       tensorboard_logs(model_name)])\n",
    "        val_acc_per_epoch = history.history['val_accuracy']\n",
    "        best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "        print(f\"best_epoch : {best_epoch}\")\n",
    "\n",
    "        hypermodel = tuner.hypermodel.build(best_hps)\n",
    "        # Retrain the model with epoch with highest val_accuracy value\n",
    "        hypermodel.fit(train_dataset,\n",
    "                       epochs=best_epoch,\n",
    "                       validation_data=val_dataset,\n",
    "                       callbacks=[stop_early,\n",
    "                                  epochs_logs(model_name),\n",
    "                                  tensorboard_logs(model_name)])\n",
    "\n",
    "        eval_result = hypermodel.evaluate(val_dataset)\n",
    "\n",
    "        hypermodel.save(f\"{MODELS}\\\\\"\n",
    "                        f\"{model_name}\"\n",
    "                        f\"_loss_{eval_result[0]}\"\n",
    "                        f\"_acc_{eval_result[1]}\"\n",
    "                        f\"_best_epoch_{best_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training(rnn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [f'{root}\\\\{dir}' for root, dirs, files in os.walk(MODELS) for dir in dirs if \"acc\" in dir]\n",
    "sort_models_per_acc = sorted(models,\n",
    "                             key=lambda x: float(x[x.find('_acc_') + 5:x.find('_best_') if 'best' in x else x.find(\n",
    "                                 '_para_') if \"_para_\" in x else None]),\n",
    "                             reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tf.keras.models.load_model(sort_models_per_acc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_model.predict(val_dataset).argmax(axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "4IABD_PA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
