{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import NoReturn, Any\n",
    "\n",
    "import keras_tuner as kt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from mlxtend.plotting import plot_confusion_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"../../\"\n",
    "MONITORING = os.path.join(BASE_PATH, \"logs\")\n",
    "DATA = os.path.join(BASE_PATH, \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = os.path.join(MONITORING, \"checkpoints\")\n",
    "CNN_CHECKPOINT_PATH = os.path.join(CHECKPOINT_PATH, \"cnn\")\n",
    "RNN_CHECKPOINT_PATH = os.path.join(CHECKPOINT_PATH, \"rnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSORBOARD_LOG_DIR = os.path.join(MONITORING, \"tensorboard_logs\")\n",
    "CNN_TENSORBOARD_LOGS = os.path.join(TENSORBOARD_LOG_DIR, \"cnn\")\n",
    "RNN_TENSORBOARD_LOGS = os.path.join(TENSORBOARD_LOG_DIR, \"rnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CSV_LOG_DIR = os.path.join(MONITORING, \"csv_logs\")\n",
    "CNN_CSV_LOGS = os.path.join(CSV_LOG_DIR, \"cnn\")\n",
    "RNN_CSV_LOGS = os.path.join(CSV_LOG_DIR, \"rnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TUNERS = os.path.join(DATA, \"tuners\")\n",
    "MODELS = os.path.join(DATA, \"models\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GPU/TPU Multithreading Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "\n",
    "    strategy = tf.distribute.experimental.TPUStrategy\n",
    "except ValueError:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print(\"Number of replicas:\", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "    gpus = tf.config.experimental.list_logical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tpu:\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(\n",
    "        tpu,\n",
    "    )\n",
    "    print(\"Running on TPU \", tpu.cluster_spec().as_dict()[\"worker\"])\n",
    "elif len(gpus) > 1:\n",
    "    strategy = tf.distribute.MultiWorkerMirroredStrategy([gpu.name for gpu in gpus])\n",
    "    print(\"Running on multiple GPUs \", [gpu.name for gpu in gpus])\n",
    "elif len(gpus) == 1:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print(\"Running on single GPU \", gpus[0].name)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print(\"Running on CPU\")\n",
    "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjustable\n",
    "BATCH_SIZE = 32  # Big batch size, small learning rate\n",
    "HEIGHT, WIDTH = 224, 224\n",
    "IMG_SIZE = (HEIGHT, WIDTH)\n",
    "IMG_FORMAT = (HEIGHT, WIDTH, 3)\n",
    "EPOCHS = 1000\n",
    "TRIALS = 20\n",
    "SEED = 949953915"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"../../data/dataset/img/mfcc\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"both\",\n",
    "    seed=SEED,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_dataset.class_names\n",
    "num_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Representation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_dataset.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = train_dataset.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_dataset = val_dataset.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1.0 / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():    \n",
    "    normalized_ds = train_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "    image_batch, labels_batch = next(iter(normalized_ds))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(hp: kt.HyperParameters) -> tf.keras.Sequential:\n",
    "    inputs = tf.keras.Input(shape=IMG_FORMAT)\n",
    "    x = tf.keras.layers.Rescaling(1.0 / 255)(inputs)\n",
    "    x = tf.keras.layers.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=3,\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "    )(x)\n",
    "    x = tf.keras.layers.MaxPooling2D()(x)\n",
    "    x = tf.keras.layers.Conv2D(\n",
    "        filters=hp.Int(\"conv_2_filter\", min_value=64, max_value=256, step=8),\n",
    "        kernel_size=3,\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "    )(x)\n",
    "    x = tf.keras.layers.MaxPooling2D()(x)\n",
    "    x = tf.keras.layers.Conv2D(\n",
    "        filters=hp.Int(\"conv_3_filter\", min_value=128, max_value=1024, step=8),\n",
    "        kernel_size=3,\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "    )(x)\n",
    "    x = tf.keras.layers.MaxPooling2D()(x)\n",
    "    x = tf.keras.layers.Dropout(\n",
    "        rate=hp.Float(\n",
    "            \"dropout_1\",\n",
    "            min_value=0.0,\n",
    "            max_value=0.5,\n",
    "            default=0.25,\n",
    "            step=0.05,\n",
    "        )\n",
    "    )(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(\n",
    "        units=hp.Int(\"dense_1_units\", min_value=32, max_value=128, step=8),\n",
    "        activation=\"relu\",\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "    )(x)\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            hp.Choice(\"learning_rate\", values=[1e-2, 1e-3])\n",
    "        ),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utilitary For Monitoring**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_early = [\n",
    "    {\n",
    "        \"monitor\": \"val_loss\",\n",
    "        \"patience\": 50,\n",
    "        \"min_delta\": 0.001,\n",
    "        \"mode\": \"min\",\n",
    "        \"verbose\": 1,\n",
    "        \"restore_best_weights\": True,\n",
    "    },\n",
    "    {\n",
    "        \"monitor\": \"val_accuracy\",\n",
    "        \"patience\": 50,\n",
    "        \"min_delta\": 0.001,\n",
    "        \"mode\": \"max\",\n",
    "        \"verbose\": 1,\n",
    "        \"restore_best_weights\": True,\n",
    "    },\n",
    "]\n",
    "stop_early = [tf.keras.callbacks.EarlyStopping(**condition) for condition in stop_early]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def path_exists(path: str) -> str:\n",
    "    if os.path.exists(path):\n",
    "        if path[-1].isdigit():\n",
    "            suffix = path[: path.rfind(\"_\")]\n",
    "            digits = int(path[path.rfind(\"_\") + 1 :])\n",
    "            path = f\"{suffix}_{digits + 1}\"\n",
    "        else:\n",
    "            path = f\"{path}_0\"\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_checkpoint(model_name):\n",
    "    return tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=path_exists(\n",
    "            f\"{globals()[f'{model_name.upper()}_CHECKPOINT_PATH']}/\"\n",
    "            f\"BS_{BATCH_SIZE}\"\n",
    "            f\"_LR_{SEED}\"\n",
    "            f\"_EPOCHS_{EPOCHS}\"\n",
    "            f\"_TRIALS_{TRIALS}\"\n",
    "        ),\n",
    "        save_weights_only=False,\n",
    "        monitor=\"val_accuracy\",\n",
    "        mode=\"max\",\n",
    "        save_best_only=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tensorboard_logs(model_name: str) -> tf.keras.callbacks.TensorBoard:\n",
    "    return tf.keras.callbacks.TensorBoard(\n",
    "        path_exists(\n",
    "            f\"{globals()[f'{model_name.upper()}_TENSORBOARD_LOGS']}/\"\n",
    "            f\"BS_{BATCH_SIZE}\"\n",
    "            f\"_LR_{SEED}\"\n",
    "            f\"_EPOCHS_{EPOCHS}\"\n",
    "            f\"_TRIALS_{TRIALS}\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def epochs_logs(model_name: str) -> tf.keras.callbacks.CSVLogger:\n",
    "    path = (\n",
    "        f\"{globals()[f'{model_name.upper()}_CSV_LOGS']}/\"\n",
    "        f\"BS_{BATCH_SIZE}\"\n",
    "        f\"_LR_{SEED}\"\n",
    "        f\"_EPOCHS_{EPOCHS}\"\n",
    "        f\"_TRIALS_{TRIALS}\"\n",
    "    )\n",
    "    return tf.keras.callbacks.CSVLogger(f\"{path_exists(path)}.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model: Any) -> NoReturn:\n",
    "    model_name = model.__name__\n",
    "    with strategy.scope():\n",
    "        tuner = kt.BayesianOptimization(\n",
    "            hypermodel=model,\n",
    "            objective=kt.Objective(\"val_accuracy\", direction=\"max\"),\n",
    "            max_trials=TRIALS,\n",
    "            overwrite=True,\n",
    "            project_name=path_exists(f\"{TUNERS}\\\\{model_name}_tuner\"),\n",
    "            directory=path_exists(\"{TUNERS}_{model_name}\"),\n",
    "        )\n",
    "\n",
    "        # Search for best hyperparameters\n",
    "        tuner.search(\n",
    "            train_dataset,\n",
    "            epochs=EPOCHS,\n",
    "            validation_data=val_dataset,\n",
    "            callbacks=[\n",
    "                stop_early,\n",
    "                model_checkpoint(model_name),\n",
    "                epochs_logs(model_name),\n",
    "                tensorboard_logs(model_name),\n",
    "            ],\n",
    "        )\n",
    "        # Get the optimal hyperparameters\n",
    "        best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "        # Build model with optimal hyperparameters\n",
    "        model = tuner.hypermodel.build(best_hps)\n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            epochs=EPOCHS,\n",
    "            validation_data=val_dataset,\n",
    "            callbacks=[\n",
    "                stop_early,\n",
    "                model_checkpoint(model_name),\n",
    "                epochs_logs(model_name),\n",
    "                tensorboard_logs(model_name),\n",
    "            ],\n",
    "        )\n",
    "        val_acc_per_epoch = history.history[\"val_accuracy\"]\n",
    "        best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "        print(f\"best_epoch : {best_epoch}\")\n",
    "\n",
    "        hypermodel = tuner.hypermodel.build(best_hps)\n",
    "        # Retrain the model with epoch with highest val_accuracy value\n",
    "        hypermodel.fit(\n",
    "            train_dataset,\n",
    "            epochs=best_epoch,\n",
    "            validation_data=val_dataset,\n",
    "            callbacks=[\n",
    "                stop_early,\n",
    "                model_checkpoint(model_name),\n",
    "                epochs_logs(model_name),\n",
    "                tensorboard_logs(model_name),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        eval_result = hypermodel.evaluate(val_dataset)\n",
    "\n",
    "        hypermodel.save(\n",
    "            f\"{MODELS}\\\\\"\n",
    "            f\"{model_name}\"\n",
    "            f\"_loss_{eval_result[0]}\"\n",
    "            f\"_acc_{eval_result[1]}\"\n",
    "            f\"_best_epoch_{best_epoch}\"\n",
    "            f\"_img_size_{IMG_SIZE}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training(cnn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    f\"{root}\\\\{dir}\"\n",
    "    for root, dirs, files in os.walk(MODELS)\n",
    "    for dir in dirs\n",
    "    if \"acc\" in dir\n",
    "]\n",
    "sort_models_per_acc = sorted(\n",
    "    models,\n",
    "    key=lambda x: float(\n",
    "        x[\n",
    "            x.find(\"_acc_\") + 5 : x.find(\"_best_\")\n",
    "            if \"best\" in x\n",
    "            else x.find(\"_para_\")\n",
    "            if \"_para_\" in x\n",
    "            else None\n",
    "        ]\n",
    "    ),\n",
    "    reverse=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tf.keras.models.load_model(sort_models_per_acc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_models_per_acc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_model.predict(val_dataset).argmax(axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = np.concatenate([y for x, y in val_dataset], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = tf.math.confusion_matrix(test_labels, predictions).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_confusion_matrix(\n",
    "    conf_mat=confusion_matrix,\n",
    "    class_names=val_dataset.class_names,\n",
    ")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "4IABD_PA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
